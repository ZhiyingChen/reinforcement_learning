# Reinforcement Learning â€” Code Implementation (Chapters 4â€“6)

æœ¬é¡¹ç›®æ—¨åœ¨ç³»ç»Ÿå¤ç°å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ï¼ˆèµµä¸–é’° Â· è¥¿æ¹–å¤§å­¦ï¼‰ä¸­ç¬¬ 4â€“6 ç« çš„ä¸»è¦ç®—æ³•ã€‚  
åç»­å°†ç»§ç»­æ‰©å±•è‡³ Chapter 7â€“10ï¼ˆTDã€Q-learningã€Function Approximationã€Policy Gradient ç­‰ï¼‰ã€‚

é¡¹ç›®ç‰¹ç‚¹ï¼š

- è‡ªå®šä¹‰ **GridWorld ç¯å¢ƒ**
- å®Œæ•´çš„ **DP + MC ä¸¤å¤§ç±»ç®—æ³•æ¡†æ¶**
- **æ—¥å¿—ç³»ç»Ÿï¼ˆPython logging + TensorBoard + Timingï¼‰**
- æ¸…æ™°çš„æ¨¡å—åŒ–ä»£ç ç»“æ„ï¼Œä¾¿äºæ‰©å±•
- æ”¯æŒç­–ç•¥å¯è§†åŒ–ã€çŠ¶æ€ä»·å€¼å¯è§†åŒ–

## âš™ï¸ How to Run
å®‰è£…ä¾èµ–
```shell
pip install -r requirements.txt
```
è¿è¡ŒæŸä¸€ä¸ªç« èŠ‚çš„å®éªŒï¼š
```shell
python main/chapter_4_1_value_iteration.py
python main/chapter_5_3_mc_epsilon_greedy.py
```

---

## ğŸ“‚ Project Structure

```
reinforcement_learning/
â”‚
â”œâ”€â”€ logs/                                  # è¿è¡Œæ—¥å¿— / TensorBoard è¾“å‡º / æ—¶é—´ç»Ÿè®¡
â”‚
â”œâ”€â”€ main/                                  # å„ç« èŠ‚å¯è¿è¡Œè„šæœ¬ï¼ˆå…¥å£ï¼‰
â”‚   â”œâ”€â”€ chapter_4_1_value_iteration.py
â”‚   â”œâ”€â”€ chapter_4_2_policy_iteration.py
â”‚   â”œâ”€â”€ chapter_4_3_truncated_policy_iteration.py
â”‚   â”œâ”€â”€ chapter_5_1_mc_basic.py
â”‚   â”œâ”€â”€ chapter_5_2_mc_exploring_starts.py
â”‚   â”œâ”€â”€ chapter_5_3_mc_epsilon_greedy.py
â”‚   â”œâ”€â”€ chapter_6_1_robbins_monro.py
â”‚   â””â”€â”€ chapter_6_2_sgd_variants.py
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”œâ”€â”€ dp_planner.py                  # DPï¼šVI / PI / Truncated PI
â”‚   â”‚   â”œâ”€â”€ mc_planner.py                  # MCï¼šBasic / ES / Îµ-greedy
â”‚   â”‚   â””â”€â”€ sa_planner.py                  # SAï¼šRM / GD / SGD / BGD / MBGD
â”‚   â”‚
â”‚   â”œâ”€â”€ domain_object/
â”‚   â”‚   â”œâ”€â”€ action.py                      # Action æšä¸¾ï¼ˆUP/DOWN/LEFT/RIGHT/STAYï¼‰
â”‚   â”‚   â””â”€â”€ transition.py                  # Transition æ•°æ®ç»“æ„
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ grid_world.py                  # GridWorld ç¯å¢ƒ
â”‚   â”‚   â”œâ”€â”€ mdp_ops.py                     # DP ç”¨ Q/V/backup å·¥å…·
â”‚   â”‚   â”œâ”€â”€ policy_ops.py                  # MC/DP é€šç”¨ç­–ç•¥å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ sa_schedules.py                # SA ç›¸å…³å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ logger_manager.py              # æ—¥å¿—ç®¡ç†ï¼ˆlogging + TensorBoardï¼‰
â”‚   â”‚   â”œâ”€â”€ timing.py                      # ä»£ç æ‰§è¡Œæ—¶é—´ç»Ÿè®¡è£…é¥°å™¨
â”‚   â”‚   â””â”€â”€ render.py                      # ç½‘æ ¼ç­–ç•¥å¯è§†åŒ–
â”‚   â”‚
â”‚   â”œâ”€â”€ grid_word.py                       # ç½‘æ ¼ä¸–ç•Œ
â”‚   â”œâ”€â”€ sa_expression.py                   # SAè¡¨è¾¾å¼
â”‚   â”‚
â””â”€â”€ test/                                  # æµ‹è¯•ç›®å½•
```
---



## ğŸ§± GridWorld Environment

`grid_world.py` å®šä¹‰äº†å¯é…ç½®çš„ MDP ç½‘æ ¼ç¯å¢ƒï¼š

- ä»»æ„å°ºå¯¸ `(height, width)`
- forbidden statesï¼ˆå¥–åŠ± -10ï¼Œå¯è®¾ä¸ºå¸æ”¶æ€ï¼‰
- target stateï¼ˆå¥–åŠ± +1ï¼Œå¸æ”¶æ€ï¼‰
- äº”ç§åŠ¨ä½œï¼šä¸Š / ä¸‹ / å·¦ / å³ / åŸåœ°
- æ”¯æŒï¼š
  - `step()` ç”¨äº MC é‡‡æ ·
  - `get_P()` ç”Ÿæˆ Gym-style MDP åŠ¨åŠ›å­¦ï¼Œç”¨äº DPï¼ˆVI/PIï¼‰

å¥–åŠ±æ¨¡å‹ã€è½¬ç§»æ¦‚ç‡å‡å¯è‡ªå®šä¹‰ã€‚

---

## ğŸ”· Chapter 4 â€” Value & Policy Iteration 

V & P ç›¸å…³ç®—æ³•ä½äºï¼š

`source/algorithms/dp_planner.py`

æä¾›ä¸‰ç§ç»å…¸ V & P  æ–¹æ³•ï¼š

### **1. Value Iteration**
å…¥å£ï¼š`main/chapter_4_1_value_iteration.py`

- åŸºäº Bellman Optimality  
- è¿­ä»£è®¡ç®— $V(s)$ï¼Œæ¯æ¬¡ä½¿ç”¨ $\max_a q_k(s,a)$  
- è¾“å‡ºæœ€ä¼˜ç­–ç•¥ $Ï€^* $ ä¸å€¼å‡½æ•° $V^*$

### **2. Policy Iteration**
å…¥å£ï¼š`main/chapter_4_2_policy_iteration.py`

æµç¨‹ï¼š

1. Policy Evaluationï¼ˆå®Œæ•´æ±‚è§£ $V_Ï€$ï¼‰
2. Policy Improvementï¼ˆè´ªå¿ƒæ”¹è¿›ï¼‰
3. ç›´åˆ°ç­–ç•¥ç¨³å®š

### **3. Truncated Policy Iteration**
å…¥å£ï¼š`main/chapter_4_3_truncated_policy_iteration.py`

- ä»…æ‰§è¡Œ **æœ‰é™æ¬¡è¯„ä¼° sweep**
- æ›´é€‚ç”¨äºå¤§è§„æ¨¡ MDP
- ä»‹äº PI ä¸ VI ä¹‹é—´çš„æŠ˜è¡·ç®—æ³•

---

## ğŸ”¶ Chapter 5 â€” Monteâ€‘Carlo (Model-Free)

MC ç›¸å…³ç®—æ³•ä½äºï¼š
`source/algorithms/mc_planner.py`

æ”¯æŒ MC ä¸‰ä»¶å¥—ï¼š

### **1. MC Basic**
å…¥å£ï¼š`main/chapter_5_1_mc_basic.py`

- å¯¹æ¯ä¸ª `(s, a)` é‡å¤é‡‡æ · episode  
- å¹³å‡å›æŠ¥ä¼°è®¡ $q(s,a)$  
- å†æ‰§è¡Œè´ªå¿ƒç­–ç•¥æ”¹è¿›

### **2. MC Exploring Starts (ES)**
å…¥å£ï¼š`main/chapter_5_2_mc_exploring_starts.py`

- æ¯ä¸ª episode éšæœºé€‰æ‹©èµ·å§‹ `(s0, a0)`
- å°½é‡ä¿è¯æ‰€æœ‰ `(s,a)` éƒ½èƒ½è¢«æ¢ç´¢
- æ¯” MC Basic æ”¶æ•›å¿«ï¼Œæ•ˆç‡é«˜

### **3. MC Îµâ€‘Greedyï¼ˆOnâ€‘policyï¼‰**
å…¥å£ï¼š`main/chapter_5_3_mc_epsilon_greedy.py`

- ä¸éœ€è¦ä¿è¯æ‰€æœ‰ `(s,a)` éƒ½èƒ½è¢«æ¢ç´¢
- è¡Œä¸ºç­–ç•¥ = ç›®æ ‡ç­–ç•¥ = $Îµ$â€‘greedy(Q)
- å¯é…ç½® $Îµ$ decay  


---

## ğŸŸ£ Chapter 6 â€” Stochastic Approximation & SGD

SA ç›¸å…³ç®—æ³•ä½äºï¼š`source/algorithms/sa_planner.py`  
æ”¯æŒä¸¤å¤§èŒƒå¼ï¼š**Robbinsâ€“Monro éšæœºé€¼è¿‘**ï¼ˆæ±‚æ ¹ï¼‰ä¸ **SGD åŠå…¶å˜ä½“**ï¼ˆä¼˜åŒ–ï¼‰ã€‚

æ ¸å¿ƒæŠ½è±¡ï¼ˆ`source/sa_expression.py`ï¼‰ï¼š

- **RootFindingOracle**ï¼šå°è£…å¸¦å™ªå£°çš„è§‚æµ‹å‡½æ•° $Ä(w,Î·) = g(w) + Î·$ï¼Œç”¨äº RM ç®—æ³•æ±‚æ ¹ $g(w)=0 $ 
- **MinimizationOracle**ï¼šå°è£…éšæœºç›®æ ‡ $f(w,X)$ ä¸æ¢¯åº¦ä¼°è®¡ï¼Œç”¨äº SGD ç±»ç®—æ³•æœ€å°åŒ– $J(w)=E[f(w,X)]$

æ­¥é•¿è°ƒåº¦ï¼ˆ`source/utils/sa_schedules.py`ï¼‰ï¼š

- **Robbinsâ€“Monro æ­¥é•¿**ï¼š$a_k = a_0 / k^Î²$ï¼Œæ»¡è¶³æ”¶æ•›å……åˆ†æ¡ä»¶ $ï¼ˆ0.5 &lt; Î² â‰¤ 1 æ—¶ Î£a_k=âˆ ä¸” Î£a_kÂ²&lt;âˆï¼‰$
- **å›ºå®šå­¦ä¹ ç‡**ï¼šç”¨äº GD å¯¹æ¯”å®éªŒ
- **çº§æ•°è¯Šæ–­**ï¼šè‡ªåŠ¨éªŒè¯æ­¥é•¿æ¡ä»¶æ˜¯å¦æ»¡è¶³

### **1. Robbinsâ€“Monro ç®—æ³•**
å…¥å£ï¼š`main/chapter_6_1_robbins_monro.py`

æ±‚è§£æ–¹ç¨‹ g(w)=0 çš„éšæœºè¿­ä»£ç®—æ³•ï¼š$w_{k+1} = w_k - a_k Â· Ä(w_k)$


- æ”¯æŒå¤šç»´/æ ‡é‡æ ¹æŸ¥æ‰¾
- å¯é…ç½®é«˜æ–¯å™ªå£° $Î· \sim N(0, ÏƒÂ²)$ æ¨¡æ‹ŸçœŸå®è§‚æµ‹å™ªå£°
- å¯é€‰æŠ•å½±æ“ä½œä¿è¯è¿­ä»£æœ‰ç•Œ
- ç¤ºä¾‹ï¼šæ±‚è§£ $tanh(w-1)=0$ï¼ŒçœŸæ ¹ $w^*=1$

æ”¶æ•›æ¡ä»¶ï¼ˆå……åˆ†æ¡ä»¶ï¼‰ï¼š
- $g(w)$ æ»¡è¶³ Lipschitz æ¡ä»¶ä¸”æ¢¯åº¦æœ‰ç•Œ$ï¼ˆ0 < c1 â‰¤ ||âˆ‡g|| â‰¤ c2ï¼‰$
- æ­¥é•¿æ»¡è¶³ $Î£a_k=âˆ, Î£a_kÂ²<âˆ$ï¼ˆé»˜è®¤ $Î²=1.0$ï¼‰
- å™ªå£°é›¶å‡å€¼ã€äºŒé˜¶çŸ©æœ‰ç•Œ

### **2. SGD å˜ä½“ï¼ˆBGD / MBGD / SGDï¼‰**
å…¥å£ï¼š`main/chapter_6_2_sgd_variants.py`

é’ˆå¯¹æœ€å°åŒ–é—®é¢˜ $J(w)=E[f(w,X)]$ï¼Œå®ç°å››ç§æ¢¯åº¦æ–¹æ³•ï¼š

| æ–¹æ³• | æ‰¹æ¬¡å¤§å° | æ­¥é•¿ç­–ç•¥ | ç‰¹ç‚¹ |
|------|---------|---------|------|
| **GD** | å…¨é‡æ•°æ® | å›ºå®š lr | ç¡®å®šæ€§æ¢¯åº¦ï¼Œä»…ä½œå¯¹æ¯”åŸºå‡† |
| **BGD** | å…¨é‡æ•°æ® | RM æ­¥é•¿ | æ¯è½®éå†å…¨æ•°æ®é›†ï¼Œç†è®ºä¿è¯æ”¶æ•› |
| **MBGD** | batch_size (e.g., 64) | RM æ­¥é•¿ | å°æ‰¹é‡éšæœºæ¢¯åº¦ï¼Œå®ç”¨æŠ˜ä¸­ |
| **SGD** | 1 | RM æ­¥é•¿ | å•æ ·æœ¬æ›´æ–°ï¼Œæ–¹å·®å¤§ä½†è®¡ç®—å¿« |

**ç»Ÿä¸€æ”¶æ•›æ¡†æ¶**ï¼š
- é™¤ GD ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡å¤–ï¼ŒBGD/MBGD/SGD å‡é‡‡ç”¨ RM æ­¥é•¿ï¼ˆ$a_k = a_0/k^Î²$ï¼‰
- åœ¨æ»¡è¶³æ ‡å‡†å‡è®¾ï¼ˆå‡¸æ€§/å¹³æ»‘æ€§/æ­¥é•¿æ¡ä»¶ï¼‰æ—¶ï¼Œ$w_k$ å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ° $âˆ‡J(w)=0$ çš„æ ¹
- æ”¯æŒæŠ•å½±æ“ä½œå¤„ç†çº¦æŸä¼˜åŒ–

**ç¤ºä¾‹ä»»åŠ¡**ï¼šå‡å€¼ä¼°è®¡  
æœ€å°åŒ– $J(w) = E[\frac{1}{2}||w-X||Â²]$ï¼Œå…¶ä¸­ $X \sim N(Î¼,ÏƒÂ²)$ï¼Œæœ€ä¼˜è§£ $w^*=Î¼$ã€‚é€šè¿‡ä¸åŒ batch size å¯¹æ¯”æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§ã€‚


---

## ğŸ“Š Logging / TensorBoard / Timing

é¡¹ç›®æä¾›å®Œæ•´æ—¥å¿—æ”¯æŒï¼š

### âœ” Python Logging  
æ—¥å¿—è¾“å‡ºåˆ°ï¼š`logs/run.log`ï¼Œç”±ï¼š`utils/logger_manager.py`ç»Ÿä¸€ç®¡ç†ã€‚

---

### âœ” TensorBoard å¯è§†åŒ–

å¯è§†åŒ–å†…å®¹åŒ…æ‹¬ï¼š

- episode returns  
- epsilon è¡°å‡  
- æœ€å¤§ Q å˜åŒ–  
- MC/DP/RM/GD æ”¶æ•›è¶‹åŠ¿  

è¿è¡Œï¼š 
```shell
tensorboard --logdir logs/
```

---

### âœ” æ—¶é—´ç»Ÿè®¡ï¼ˆTimingï¼‰

`timing.py` æä¾›ï¼š`@record_time_decorator("task_name")` è‡ªåŠ¨è®°å½•æ¯ä¸€æ®µä»£ç çš„è¿è¡Œæ—¶é—´è‡³log


---

## ğŸš€ To Be Continued (Chapters 7â€“10)
æœ¬ä»“åº“ä»åœ¨æŒç»­å¼€å‘ï¼Œæœªæ¥å°†åŠ å…¥ Chapter 7-10 çš„éƒ¨åˆ† ï¼ˆâ³ TODOï¼‰
- Chapter 7: Temporal-difference learning
- Chapter 8: Value function approximation
- Chapter 9: Policy Gradient Methods
- Chapter 10: Actor Critic

## ğŸ™Œ Acknowledgement
æœ¬é¡¹ç›®ç”± Zhiying Chen ä¸»å¯¼å¼€å‘ï¼Œ ç®—æ³•ä¸ä»£ç è®¾è®¡ç”± M365 Copilot ååŠ©å®Œå–„ã€‚