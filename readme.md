# Reinforcement Learning â€” Code Implementation (Chapters 4â€“5)

æœ¬é¡¹ç›®æ—¨åœ¨ç³»ç»Ÿå¤ç°å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ï¼ˆèµµä¸–é’° Â· è¥¿æ¹–å¤§å­¦ï¼‰ä¸­ç¬¬ 4â€“5 ç« çš„ä¸»è¦ç®—æ³•ã€‚  
åç»­å°†ç»§ç»­æ‰©å±•è‡³ Chapter 6â€“10ï¼ˆTDã€Q-learningã€Function Approximationã€Policy Gradient ç­‰ï¼‰ã€‚

é¡¹ç›®ç‰¹ç‚¹ï¼š

- è‡ªå®šä¹‰ **GridWorld ç¯å¢ƒ**
- å®Œæ•´çš„ **DP + MC ä¸¤å¤§ç±»ç®—æ³•æ¡†æ¶**
- **æ—¥å¿—ç³»ç»Ÿï¼ˆPython logging + TensorBoard + Timingï¼‰**
- æ¸…æ™°çš„æ¨¡å—åŒ–ä»£ç ç»“æ„ï¼Œä¾¿äºæ‰©å±•
- æ”¯æŒç­–ç•¥å¯è§†åŒ–ã€çŠ¶æ€ä»·å€¼å¯è§†åŒ–

## âš™ï¸ How to Run
å®‰è£…ä¾èµ–
```shell
pip install -r requirements.txt
```
è¿è¡ŒæŸä¸€ä¸ªç« èŠ‚çš„å®éªŒï¼š
```shell
python main/chapter_4_1_value_iteration.py
python main/chapter_5_3_mc_epsilon_greedy.py
```

---

## ğŸ“‚ Project Structure

```
reinforcement_learning/
â”‚
â”œâ”€â”€ logs/                                  # è¿è¡Œæ—¥å¿— / TensorBoard è¾“å‡º / æ—¶é—´ç»Ÿè®¡
â”‚
â”œâ”€â”€ main/                                  # å„ç« èŠ‚å¯è¿è¡Œè„šæœ¬ï¼ˆå…¥å£ï¼‰
â”‚   â”œâ”€â”€ chapter_4_1_value_iteration.py
â”‚   â”œâ”€â”€ chapter_4_2_policy_iteration.py
â”‚   â”œâ”€â”€ chapter_4_3_truncated_policy_iteration.py
â”‚   â”œâ”€â”€ chapter_5_1_mc_basic.py
â”‚   â”œâ”€â”€ chapter_5_2_mc_exploring_starts.py
â”‚   â””â”€â”€ chapter_5_3_mc_epsilon_greedy.py
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”œâ”€â”€ dp_planner.py                  # DPï¼šVI / PI / Truncated PI
â”‚   â”‚   â””â”€â”€ mc_planner.py                  # MCï¼šBasic / ES / Îµ-greedy
â”‚   â”‚
â”‚   â”œâ”€â”€ domain_object/
â”‚   â”‚   â”œâ”€â”€ action.py                      # Action æšä¸¾ï¼ˆUP/DOWN/LEFT/RIGHT/STAYï¼‰
â”‚   â”‚   â””â”€â”€ transition.py                  # Transition æ•°æ®ç»“æ„
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ grid_world.py                  # GridWorld ç¯å¢ƒ
â”‚   â”‚   â”œâ”€â”€ mdp_ops.py                     # DP ç”¨ Q/V/backup å·¥å…·
â”‚   â”‚   â”œâ”€â”€ policy_ops.py                  # MC/DP é€šç”¨ç­–ç•¥å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ logger_manager.py              # æ—¥å¿—ç®¡ç†ï¼ˆlogging + TensorBoardï¼‰
â”‚   â”‚   â”œâ”€â”€ timing.py                      # ä»£ç æ‰§è¡Œæ—¶é—´ç»Ÿè®¡è£…é¥°å™¨
â”‚   â”‚   â””â”€â”€ render.py                      # ç½‘æ ¼ç­–ç•¥å¯è§†åŒ–
â”‚
â””â”€â”€ test/                                  # æµ‹è¯•ç›®å½•
```
---



## ğŸ§± GridWorld Environment

`grid_world.py` å®šä¹‰äº†å¯é…ç½®çš„ MDP ç½‘æ ¼ç¯å¢ƒï¼š

- ä»»æ„å°ºå¯¸ `(height, width)`
- forbidden statesï¼ˆå¥–åŠ± -10ï¼Œå¯è®¾ä¸ºå¸æ”¶æ€ï¼‰
- target stateï¼ˆå¥–åŠ± +1ï¼Œå¸æ”¶æ€ï¼‰
- äº”ç§åŠ¨ä½œï¼šä¸Š / ä¸‹ / å·¦ / å³ / åŸåœ°
- æ”¯æŒï¼š
  - `step()` ç”¨äº MC é‡‡æ ·
  - `get_P()` ç”Ÿæˆ Gym-style MDP åŠ¨åŠ›å­¦ï¼Œç”¨äº DPï¼ˆVI/PIï¼‰

å¥–åŠ±æ¨¡å‹ã€è½¬ç§»æ¦‚ç‡å‡å¯è‡ªå®šä¹‰ã€‚

---

## ğŸ”· Chapter 4 â€” Value & Policy Iteration 

V & P ç›¸å…³ç®—æ³•ä½äºï¼š

`source/algorithms/dp_planner.py`

æä¾›ä¸‰ç§ç»å…¸ V & P  æ–¹æ³•ï¼š

### **1. Value Iteration**
å…¥å£ï¼š`main/chapter_4_1_value_iteration.py`

- åŸºäº Bellman Optimality  
- è¿­ä»£è®¡ç®— V(s)ï¼Œæ¯æ¬¡ä½¿ç”¨ `max_a q_k(s,a)`  
- è¾“å‡ºæœ€ä¼˜ç­–ç•¥ Ï€\* ä¸å€¼å‡½æ•° V\*

### **2. Policy Iteration**
å…¥å£ï¼š`main/chapter_4_2_policy_iteration.py`

æµç¨‹ï¼š

1. Policy Evaluationï¼ˆå®Œæ•´æ±‚è§£ V^Ï€ï¼‰
2. Policy Improvementï¼ˆè´ªå¿ƒæ”¹è¿›ï¼‰
3. ç›´åˆ°ç­–ç•¥ç¨³å®š

### **3. Truncated Policy Iteration**
å…¥å£ï¼š`main/chapter_4_3_truncated_policy_iteration.py`

- ä»…æ‰§è¡Œ **æœ‰é™æ¬¡è¯„ä¼° sweep**
- æ›´é€‚ç”¨äºå¤§è§„æ¨¡ MDP
- ä»‹äº PI ä¸ VI ä¹‹é—´çš„æŠ˜è¡·ç®—æ³•

---

## ğŸ”¶ Chapter 5 â€” Monteâ€‘Carlo (Model-Free)

MC ç›¸å…³ç®—æ³•ä½äºï¼š
`source/algorithms/mc_planner.py`

æ”¯æŒ MC ä¸‰ä»¶å¥—ï¼š

### **1. MC Basic**
å…¥å£ï¼š`main/chapter_5_1_mc_basic.py`

- å¯¹æ¯ä¸ª (s, a) é‡å¤é‡‡æ · episode  
- å¹³å‡å›æŠ¥ä¼°è®¡ q(s,a)  
- å†æ‰§è¡Œè´ªå¿ƒç­–ç•¥æ”¹è¿›

### **2. MC Exploring Starts (ES)**
å…¥å£ï¼š`main/chapter_5_2_mc_exploring_starts.py`

- æ¯ä¸ª episode éšæœºé€‰æ‹©èµ·å§‹ `(s0, a0)`
- å°½é‡ä¿è¯æ‰€æœ‰ (s,a) éƒ½èƒ½è¢«æ¢ç´¢
- æ¯” MC Basic æ”¶æ•›å¿«ï¼Œæ•ˆç‡é«˜

### **3. MC Îµâ€‘Greedyï¼ˆOnâ€‘policyï¼‰**
å…¥å£ï¼š`main/chapter_5_3_mc_epsilon_greedy.py`

- ä¸éœ€è¦ä¿è¯æ‰€æœ‰ (s,a) éƒ½èƒ½è¢«æ¢ç´¢
- è¡Œä¸ºç­–ç•¥ = ç›®æ ‡ç­–ç•¥ = Îµâ€‘greedy(Q)
- å¯é…ç½® epsilon decay  


---

## ğŸ“Š Logging / TensorBoard / Timing

é¡¹ç›®æä¾›å®Œæ•´æ—¥å¿—æ”¯æŒï¼š

### âœ” Python Logging  
æ—¥å¿—è¾“å‡ºåˆ°ï¼š`logs/run.log`ï¼Œç”±ï¼š`utils/logger_manager.py`ç»Ÿä¸€ç®¡ç†ã€‚

---

### âœ” TensorBoard å¯è§†åŒ–

å¯è§†åŒ–å†…å®¹åŒ…æ‹¬ï¼š

- episode returns  
- epsilon è¡°å‡  
- æœ€å¤§ Q å˜åŒ–  
- MC/DP æ”¶æ•›è¶‹åŠ¿  

è¿è¡Œï¼š 
```shell
tensorboard --logdir logs/
```

---

### âœ” æ—¶é—´ç»Ÿè®¡ï¼ˆTimingï¼‰

`timing.py` æä¾›ï¼š`@record_time_decorator("task_name")` è‡ªåŠ¨è®°å½•æ¯ä¸€æ®µä»£ç çš„è¿è¡Œæ—¶é—´è‡³log


---

## ğŸš€ To Be Continued (Chapters 6â€“10)
æœ¬ä»“åº“ä»åœ¨æŒç»­å¼€å‘ï¼Œæœªæ¥å°†åŠ å…¥ Chapter 6-10 çš„éƒ¨åˆ† ï¼ˆâ³ TODOï¼‰

## ğŸ™Œ Acknowledgement
æœ¬é¡¹ç›®ç”± Zhiying Chen ä¸»å¯¼å¼€å‘ï¼Œ ç®—æ³•ä¸ä»£ç è®¾è®¡ç”± M365 Copilot ååŠ©å®Œå–„ã€‚